{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dc9cff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:18.301004Z",
     "iopub.status.busy": "2024-11-21T21:50:18.300688Z",
     "iopub.status.idle": "2024-11-21T21:50:34.868074Z",
     "shell.execute_reply": "2024-11-21T21:50:34.867041Z"
    },
    "papermill": {
     "duration": 16.574424,
     "end_time": "2024-11-21T21:50:34.870794",
     "exception": false,
     "start_time": "2024-11-21T21:50:18.296370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AMC'...\r\n",
      "remote: Enumerating objects: 1399, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (419/419), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (162/162), done.\u001b[K\r\n",
      "remote: Total 1399 (delta 223), reused 390 (delta 223), pack-reused 980 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (1399/1399), 387.41 MiB | 30.81 MiB/s, done.\r\n",
      "Resolving deltas: 100% (386/386), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jclugor/AMC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a173e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:34.884085Z",
     "iopub.status.busy": "2024-11-21T21:50:34.883781Z",
     "iopub.status.idle": "2024-11-21T21:50:34.889981Z",
     "shell.execute_reply": "2024-11-21T21:50:34.889110Z"
    },
    "papermill": {
     "duration": 0.014667,
     "end_time": "2024-11-21T21:50:34.891702",
     "exception": false,
     "start_time": "2024-11-21T21:50:34.877035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/AMC\n"
     ]
    }
   ],
   "source": [
    "%cd 'AMC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648575f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:34.903368Z",
     "iopub.status.busy": "2024-11-21T21:50:34.903097Z",
     "iopub.status.idle": "2024-11-21T21:50:46.793825Z",
     "shell.execute_reply": "2024-11-21T21:50:46.793108Z"
    },
    "papermill": {
     "duration": 11.898828,
     "end_time": "2024-11-21T21:50:46.795850",
     "exception": false,
     "start_time": "2024-11-21T21:50:34.897022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mltools\n",
    "from rmldataset2016 import load_data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdcad1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:46.809643Z",
     "iopub.status.busy": "2024-11-21T21:50:46.809134Z",
     "iopub.status.idle": "2024-11-21T21:50:49.556811Z",
     "shell.execute_reply": "2024-11-21T21:50:49.556030Z"
    },
    "papermill": {
     "duration": 2.756107,
     "end_time": "2024-11-21T21:50:49.558762",
     "exception": false,
     "start_time": "2024-11-21T21:50:46.802655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(f'/kaggle/working/AMC/1DCNN-PF')) # cant import from module starting with number\n",
    "sys.path.append(os.path.abspath(f'/kaggle/working/AMC/PET-CGDNN'))\n",
    "sys.path.append(os.path.abspath(f'/kaggle/working/AMC/IC-AMCNet'))\n",
    "sys.path.append(os.path.abspath(f'/kaggle/working/AMC/TAD'))\n",
    "from DCNNPF import DLmodel\n",
    "from CGDNet.CGDNN import CGDNN\n",
    "# from CLDNN.CLDNNLikeModel import CLDNNLikeModel\n",
    "import CLDNN.CLDNNLikeModel as cldnn\n",
    "import CLDNN2.CLDNNLikeModel as cldnn2\n",
    "from CNN1.CNN2Model import CNN2Model\n",
    "from CNN2.CNN2 import CNN2\n",
    "from DAE.DAE import DAE\n",
    "from DenseNet.DenseNet import DenseNet\n",
    "from GRU2.GRUModel import GRUModel\n",
    "from ICAMC import ICAMC\n",
    "from LSTM2.CuDNNLSTMModel import LSTMModel\n",
    "from MCLDNN.MCLDNN import MCLDNN\n",
    "from MCNET.MCNET import MCNET\n",
    "from PETCGDNN import PETCGDNN\n",
    "from ResNet.ResNet import ResNet\n",
    "from TAD import MCLDNN_VGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c4c1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:49.571666Z",
     "iopub.status.busy": "2024-11-21T21:50:49.571136Z",
     "iopub.status.idle": "2024-11-21T21:50:49.575957Z",
     "shell.execute_reply": "2024-11-21T21:50:49.575312Z"
    },
    "papermill": {
     "duration": 0.013043,
     "end_time": "2024-11-21T21:50:49.577547",
     "exception": false,
     "start_time": "2024-11-21T21:50:49.564504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dict = {'1DCNN-PF': DLmodel,\n",
    "               'CGDNet': CGDNN,\n",
    "               'CLDNN': cldnn.CLDNNLikeModel,\n",
    "               'CLDNN2': cldnn2.CLDNNLikeModel,\n",
    "               'CNN1': CNN2Model,\n",
    "               'CNN2': CNN2,\n",
    "               'DAE': DAE,\n",
    "               'DenseNet': DenseNet,\n",
    "               'GRU2': GRUModel,\n",
    "               'IC-AMCNet': ICAMC,\n",
    "               'LSTM2': LSTMModel,\n",
    "               'MCLDNN': MCLDNN,\n",
    "               'MCNET': MCNET,\n",
    "               'PET-CGDNN': PETCGDNN,\n",
    "               'ResNet': ResNet,\n",
    "               'TAD': MCLDNN_VGN.MCLDNN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9651167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:49.589855Z",
     "iopub.status.busy": "2024-11-21T21:50:49.589591Z",
     "iopub.status.idle": "2024-11-21T21:50:57.147438Z",
     "shell.execute_reply": "2024-11-21T21:50:57.146600Z"
    },
    "papermill": {
     "duration": 7.566351,
     "end_time": "2024-11-21T21:50:57.149476",
     "exception": false,
     "start_time": "2024-11-21T21:50:49.583125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = []\n",
    "for i, idx_to_load in enumerate([\"train_idx\", \"val_idx\", \"test_idx\"]):\n",
    "    with open(f'/kaggle/input/rml201610a-idx/{idx_to_load}.pkl', 'rb') as file:\n",
    "        idx.append(pickle.load(file))\n",
    "(mods,snrs,lbl),(X_train,Y_train),(X_val,Y_val),(X_test,Y_test),(train_idx,val_idx,test_idx) = load_data(idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b532de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:57.162013Z",
     "iopub.status.busy": "2024-11-21T21:50:57.161717Z",
     "iopub.status.idle": "2024-11-21T21:50:57.165451Z",
     "shell.execute_reply": "2024-11-21T21:50:57.164752Z"
    },
    "papermill": {
     "duration": 0.011801,
     "end_time": "2024-11-21T21:50:57.167055",
     "exception": false,
     "start_time": "2024-11-21T21:50:57.155254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_epoch = 1000\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e273d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:57.179204Z",
     "iopub.status.busy": "2024-11-21T21:50:57.178939Z",
     "iopub.status.idle": "2024-11-21T21:50:57.191179Z",
     "shell.execute_reply": "2024-11-21T21:50:57.190520Z"
    },
    "papermill": {
     "duration": 0.020196,
     "end_time": "2024-11-21T21:50:57.192755",
     "exception": false,
     "start_time": "2024-11-21T21:50:57.172559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def rotate_matrix(theta):\n",
    "    m = np.zeros((2, 2))\n",
    "    m[0, 0] = np.cos(theta)\n",
    "    m[0, 1] = -np.sin(theta)\n",
    "    m[1, 0] = np.sin(theta)\n",
    "    m[1, 1] = np.cos(theta)\n",
    "    print(m)\n",
    "    return m\n",
    "\n",
    "def Rotate_DA(x, y):\n",
    "    N, L, C = np.shape(x)\n",
    "    x_rotate1 = np.matmul(x, rotate_matrix(np.pi/2))\n",
    "    x_rotate2 = np.matmul(x, rotate_matrix(np.pi))\n",
    "    x_rotate3 = np.matmul(x, rotate_matrix(3 * np.pi / 2))\n",
    "\n",
    "    x_DA = np.vstack((x, x_rotate1, x_rotate2, x_rotate3))\n",
    "\n",
    "    y_DA = np.tile(y, (1, 4)).T.reshape(-1)\n",
    "    return x_DA, y_DA\n",
    "\n",
    "\n",
    "def norm_pad_zeros(X_train,nsamples):\n",
    "    print (\"Pad:\", X_train.shape)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train[i,:,0] = X_train[i,:,0]/np.linalg.norm(X_train[i,:,0],2)\n",
    "    return X_train\n",
    "\n",
    "def l2_normalize(x, axis=-1):\n",
    "    y = np.sum(x ** 2, axis, keepdims=True)\n",
    "    return x / np.sqrt(y)\n",
    "    \n",
    "\n",
    "def to_amp_phase(X_train,X_val,X_test,nsamples):\n",
    "    X_train_cmplx = X_train[:,0,:] + 1j* X_train[:,1,:]\n",
    "    X_val_cmplx = X_val[:,0,:] + 1j* X_val[:,1,:]\n",
    "    X_test_cmplx = X_test[:,0,:] + 1j* X_test[:,1,:]\n",
    "    \n",
    "    X_train_amp = np.abs(X_train_cmplx)\n",
    "    X_train_ang = np.arctan2(X_train[:,1,:],X_train[:,0,:])/np.pi\n",
    "    \n",
    "    \n",
    "    X_train_amp = np.reshape(X_train_amp,(-1,1,nsamples))\n",
    "    X_train_ang = np.reshape(X_train_ang,(-1,1,nsamples))\n",
    "    \n",
    "    X_train = np.concatenate((X_train_amp,X_train_ang), axis=1) \n",
    "    X_train = np.transpose(np.array(X_train),(0,2,1))\n",
    "\n",
    "    X_val_amp = np.abs(X_val_cmplx)\n",
    "    X_val_ang = np.arctan2(X_val[:,1,:],X_val[:,0,:])/np.pi\n",
    "    \n",
    "    \n",
    "    X_val_amp = np.reshape(X_val_amp,(-1,1,nsamples))\n",
    "    X_val_ang = np.reshape(X_val_ang,(-1,1,nsamples))\n",
    "    \n",
    "    X_val = np.concatenate((X_val_amp,X_val_ang), axis=1) \n",
    "    X_val = np.transpose(np.array(X_val),(0,2,1))\n",
    "    \n",
    "    X_test_amp = np.abs(X_test_cmplx)\n",
    "    X_test_ang = np.arctan2(X_test[:,1,:],X_test[:,0,:])/np.pi\n",
    "    \n",
    "    \n",
    "    X_test_amp = np.reshape(X_test_amp,(-1,1,nsamples))\n",
    "    X_test_ang = np.reshape(X_test_ang,(-1,1,nsamples))\n",
    "    \n",
    "    X_test = np.concatenate((X_test_amp,X_test_ang), axis=1) \n",
    "    X_test = np.transpose(np.array(X_test),(0,2,1))\n",
    "    return (X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c43836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:57.204426Z",
     "iopub.status.busy": "2024-11-21T21:50:57.204128Z",
     "iopub.status.idle": "2024-11-21T21:50:57.207652Z",
     "shell.execute_reply": "2024-11-21T21:50:57.206962Z"
    },
    "papermill": {
     "duration": 0.011068,
     "end_time": "2024-11-21T21:50:57.209168",
     "exception": false,
     "start_time": "2024-11-21T21:50:57.198100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data augmentation with rotation\n",
    "#X_train = X_train.copy().transpose((0, 2, 1))\n",
    "#Y_train = np.argmax(Y_train, axis=1)\n",
    "#X_train, Y_train = Rotate_DA(X_train, Y_train)\n",
    "#X_train = X_train.transpose((0, 2, 1))\n",
    "#Y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b893c5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T21:50:57.220880Z",
     "iopub.status.busy": "2024-11-21T21:50:57.220658Z",
     "iopub.status.idle": "2024-11-21T23:57:53.249989Z",
     "shell.execute_reply": "2024-11-21T23:57:53.248716Z"
    },
    "papermill": {
     "duration": 7616.037783,
     "end_time": "2024-11-21T23:57:53.252267",
     "exception": false,
     "start_time": "2024-11-21T21:50:57.214484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.38857, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 2.38857 to 2.27779, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 2.27779 to 2.14849, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 2.14849 to 1.98085, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.98085 to 1.77780, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.77780 to 1.73965, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.73965 to 1.65582, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.65582 to 1.60957, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.60957 to 1.57234, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.57234 to 1.56375, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 1.56375 to 1.54395, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.54395 to 1.53028, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 1.53028 to 1.51621, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 14: val_loss improved from 1.51621 to 1.48643, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 15: val_loss improved from 1.48643 to 1.47769, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 1.47769 to 1.39948, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.39948 to 1.37498, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 1.37498 to 1.33372, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 19: val_loss improved from 1.33372 to 1.31625, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 1.31625 to 1.27805, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 1.27805 to 1.27186, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 1.27186 to 1.25611, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 1.25611 to 1.24086, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.24086\n",
      "\n",
      "Epoch 25: val_loss improved from 1.24086 to 1.21967, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.21967\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.21967\n",
      "\n",
      "Epoch 28: val_loss improved from 1.21967 to 1.21179, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 1.21179 to 1.20974, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 1.20974 to 1.19517, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.19517\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.19517\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.19517\n",
      "\n",
      "Epoch 34: val_loss improved from 1.19517 to 1.19508, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 1.19508 to 1.18182, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.18182\n",
      "\n",
      "Epoch 37: val_loss improved from 1.18182 to 1.17776, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.17776\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.17776\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.17776\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.17776\n",
      "\n",
      "Epoch 42: val_loss improved from 1.17776 to 1.17553, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.17553\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.17553\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.17553\n",
      "\n",
      "Epoch 46: val_loss improved from 1.17553 to 1.17080, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.17080\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.17080\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.17080\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.17080\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.17080\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 52: val_loss improved from 1.17080 to 1.16809, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 53: val_loss improved from 1.16809 to 1.16521, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 54: val_loss improved from 1.16521 to 1.16269, saving model to results/GRU2/weights.keras\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.16269\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 84: early stopping\n",
      "Finished training GRU2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "  adding: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732226742.166491      76 service.cc:145] XLA service 0x7b281c0514e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732226742.166550      76 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2024-11-21 22:05:47.236843: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng12{k11=2} for conv (f32[512,128,1,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[512,128,1,71]{3,2,1,0}, f32[128,128,1,8]{3,2,1,0}, f32[128]{0}), window={size=1x8}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-11-21 22:05:47.405924: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.169200684s\n",
      "Trying algorithm eng12{k11=2} for conv (f32[512,128,1,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[512,128,1,71]{3,2,1,0}, f32[128,128,1,8]{3,2,1,0}, f32[128]{0}), window={size=1x8}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "I0000 00:00:1732226751.263376      76 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-21 22:06:05.577752: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng12{k11=2} for conv (f32[480,128,1,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[480,128,1,71]{3,2,1,0}, f32[128,128,1,8]{3,2,1,0}, f32[128]{0}), window={size=1x8}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-11-21 22:06:05.667425: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.089781415s\n",
      "Trying algorithm eng12{k11=2} for conv (f32[480,128,1,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[480,128,1,71]{3,2,1,0}, f32[128,128,1,8]{3,2,1,0}, f32[128]{0}), window={size=1x8}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.87616, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.87616 to 1.69200, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.69200 to 1.57491, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.57491 to 1.45684, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.45684 to 1.40180, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.40180 to 1.37083, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.37083\n",
      "\n",
      "Epoch 8: val_loss improved from 1.37083 to 1.35470, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.35470\n",
      "\n",
      "Epoch 10: val_loss improved from 1.35470 to 1.32043, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.32043\n",
      "\n",
      "Epoch 12: val_loss improved from 1.32043 to 1.31238, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.31238\n",
      "\n",
      "Epoch 14: val_loss improved from 1.31238 to 1.30668, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.30668\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.30668\n",
      "\n",
      "Epoch 17: val_loss improved from 1.30668 to 1.29642, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 1.29642 to 1.29085, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.29085\n",
      "\n",
      "Epoch 20: val_loss improved from 1.29085 to 1.27858, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.27858\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.27858\n",
      "\n",
      "Epoch 23: val_loss improved from 1.27858 to 1.27556, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.27556\n",
      "\n",
      "Epoch 25: val_loss improved from 1.27556 to 1.25657, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 1.25657 to 1.25397, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 27: val_loss improved from 1.25397 to 1.24930, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.24930\n",
      "\n",
      "Epoch 29: val_loss improved from 1.24930 to 1.23635, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 1.23635 to 1.23010, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 1.23010 to 1.22940, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 1.22940 to 1.22757, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.22757\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.22757\n",
      "\n",
      "Epoch 35: val_loss improved from 1.22757 to 1.22562, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 36: val_loss improved from 1.22562 to 1.21924, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.21924\n",
      "\n",
      "Epoch 38: val_loss improved from 1.21924 to 1.21252, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.21252\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.21252\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.21252\n",
      "\n",
      "Epoch 42: val_loss improved from 1.21252 to 1.20867, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.20867\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.20867\n",
      "\n",
      "Epoch 45: val_loss improved from 1.20867 to 1.20817, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 46: val_loss improved from 1.20817 to 1.20304, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.20304\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.20304\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.20304\n",
      "\n",
      "Epoch 50: val_loss improved from 1.20304 to 1.19737, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.19737\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.19737\n",
      "\n",
      "Epoch 53: val_loss improved from 1.19737 to 1.19708, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.19708\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.19708\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.19708\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.19708\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.19708\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 59: val_loss improved from 1.19708 to 1.19500, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.19500\n",
      "\n",
      "Epoch 61: val_loss improved from 1.19500 to 1.19328, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.19328\n",
      "\n",
      "Epoch 63: val_loss improved from 1.19328 to 1.19116, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.19116\n",
      "\n",
      "Epoch 65: val_loss improved from 1.19116 to 1.18675, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.18675\n",
      "\n",
      "Epoch 67: val_loss improved from 1.18675 to 1.18400, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.18400\n",
      "\n",
      "Epoch 81: val_loss improved from 1.18400 to 1.18287, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.18287\n",
      "\n",
      "Epoch 83: val_loss improved from 1.18287 to 1.18239, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.18239\n",
      "\n",
      "Epoch 85: val_loss improved from 1.18239 to 1.18094, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.18094\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.18094\n",
      "\n",
      "Epoch 88: val_loss improved from 1.18094 to 1.18044, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.18044\n",
      "\n",
      "Epoch 110: val_loss improved from 1.18044 to 1.18034, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 111: val_loss improved from 1.18034 to 1.18023, saving model to results/IC-AMCNet/weights.keras\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.18023\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 141: early stopping\n",
      "Finished training IC-AMCNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "Pad: (132000, 128, 2)\n",
      "Pad: (44000, 128, 2)\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.07432, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 2.07432 to 1.93583, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.93583 to 1.72754, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.72754 to 1.68075, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.68075 to 1.62748, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.62748 to 1.58613, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.58613 to 1.56825, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.56825 to 1.50187, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.50187 to 1.47419, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.47419 to 1.46180, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 1.46180 to 1.43881, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.43881 to 1.42962, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.42962\n",
      "\n",
      "Epoch 14: val_loss improved from 1.42962 to 1.41225, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 15: val_loss improved from 1.41225 to 1.38541, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 1.38541 to 1.37612, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.37612 to 1.36942, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 1.36942 to 1.33303, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 19: val_loss improved from 1.33303 to 1.29478, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 1.29478 to 1.26948, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 1.26948 to 1.23522, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 1.23522 to 1.23401, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 1.23401 to 1.22128, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.22128\n",
      "\n",
      "Epoch 25: val_loss improved from 1.22128 to 1.21042, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.21042\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.21042\n",
      "\n",
      "Epoch 28: val_loss improved from 1.21042 to 1.20919, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 1.20919 to 1.20127, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 1.20127 to 1.19570, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 1.19570 to 1.19543, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 1.19543 to 1.19203, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.19203\n",
      "\n",
      "Epoch 34: val_loss improved from 1.19203 to 1.18925, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 1.18925 to 1.18822, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 36: val_loss improved from 1.18822 to 1.18652, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 37: val_loss improved from 1.18652 to 1.17487, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 38: val_loss improved from 1.17487 to 1.16547, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.16547\n",
      "\n",
      "Epoch 40: val_loss improved from 1.16547 to 1.16458, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 41: val_loss improved from 1.16458 to 1.15759, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.15759\n",
      "\n",
      "Epoch 43: val_loss improved from 1.15759 to 1.15135, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.15135\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.15135\n",
      "\n",
      "Epoch 46: val_loss improved from 1.15135 to 1.15111, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 47: val_loss improved from 1.15111 to 1.13716, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.13716\n",
      "\n",
      "Epoch 49: val_loss improved from 1.13716 to 1.12570, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.12570\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.12570\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.12570\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.12570\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.12570\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 55: val_loss improved from 1.12570 to 1.12057, saving model to results/LSTM2/weights.keras\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.12057\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 85: early stopping\n",
      "Finished training LSTM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "  adding: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.07618, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 2.07618 to 1.96583, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.96583 to 1.71738, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.71738 to 1.57512, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.57512 to 1.53469, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.53469 to 1.49533, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.49533\n",
      "\n",
      "Epoch 8: val_loss improved from 1.49533 to 1.40725, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.40725 to 1.34801, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.34801 to 1.32659, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 1.32659 to 1.31637, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.31637 to 1.31404, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 1.31404 to 1.29657, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.29657\n",
      "\n",
      "Epoch 15: val_loss improved from 1.29657 to 1.26588, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 1.26588 to 1.23858, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.23858\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.23858\n",
      "\n",
      "Epoch 19: val_loss improved from 1.23858 to 1.22374, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.22374\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.22374\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.22374\n",
      "\n",
      "Epoch 23: val_loss improved from 1.22374 to 1.21183, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.21183\n",
      "\n",
      "Epoch 25: val_loss improved from 1.21183 to 1.20099, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.20099\n",
      "\n",
      "Epoch 27: val_loss improved from 1.20099 to 1.19002, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.19002\n",
      "\n",
      "Epoch 29: val_loss improved from 1.19002 to 1.18924, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 1.18924 to 1.18407, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.18407\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.18407\n",
      "\n",
      "Epoch 33: val_loss improved from 1.18407 to 1.17630, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.17630\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.17630\n",
      "\n",
      "Epoch 36: val_loss improved from 1.17630 to 1.17574, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.17574\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.17574\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.17574\n",
      "\n",
      "Epoch 40: val_loss improved from 1.17574 to 1.16857, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.16857\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.16857\n",
      "\n",
      "Epoch 43: val_loss improved from 1.16857 to 1.16603, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.16603\n",
      "\n",
      "Epoch 45: val_loss improved from 1.16603 to 1.16206, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.16206\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.16206\n",
      "\n",
      "Epoch 48: val_loss improved from 1.16206 to 1.16002, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 49: val_loss improved from 1.16002 to 1.15148, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.15148\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.15148\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.15148\n",
      "\n",
      "Epoch 53: val_loss improved from 1.15148 to 1.14921, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 54: val_loss improved from 1.14921 to 1.14678, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 55: val_loss improved from 1.14678 to 1.14243, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 56: val_loss improved from 1.14243 to 1.12707, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.12707\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.12707\n",
      "\n",
      "Epoch 59: val_loss improved from 1.12707 to 1.11613, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 60: val_loss improved from 1.11613 to 1.09814, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.09814\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.09814\n",
      "\n",
      "Epoch 63: val_loss improved from 1.09814 to 1.09110, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.09110\n",
      "\n",
      "Epoch 65: val_loss improved from 1.09110 to 1.08908, saving model to results/MCLDNN/weights.keras\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.08908\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 95: early stopping\n",
      "Finished training MCLDNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/train_loss.txt (deflated 57%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/total_loss.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/val_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/val_acc.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/total_acc.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/train_acc.txt (deflated 57%)\r\n",
      "  adding: kaggle/working/AMC/results/MCLDNN/weights.keras (deflated 8%)\r\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.86834, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.86834 to 1.58170, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.58170 to 1.47296, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.47296 to 1.38646, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.38646 to 1.35755, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.35755 to 1.33111, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.33111\n",
      "\n",
      "Epoch 8: val_loss improved from 1.33111 to 1.32773, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.32773 to 1.28667, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.28667 to 1.28572, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.28572\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.28572\n",
      "\n",
      "Epoch 13: val_loss improved from 1.28572 to 1.26887, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 14: val_loss improved from 1.26887 to 1.25484, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.25484\n",
      "\n",
      "Epoch 16: val_loss improved from 1.25484 to 1.25146, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.25146 to 1.24976, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.24976\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.24976\n",
      "\n",
      "Epoch 20: val_loss improved from 1.24976 to 1.24321, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 1.24321 to 1.24088, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.24088\n",
      "\n",
      "Epoch 23: val_loss improved from 1.24088 to 1.24057, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 1.24057 to 1.23657, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 1.23657 to 1.23047, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 1.23047 to 1.22875, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.22875\n",
      "\n",
      "Epoch 28: val_loss improved from 1.22875 to 1.21705, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.21705\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.21705\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.21705\n",
      "\n",
      "Epoch 32: val_loss improved from 1.21705 to 1.20768, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.20768\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.20768\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.20768\n",
      "\n",
      "Epoch 36: val_loss improved from 1.20768 to 1.20257, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.20257\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.20257\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.20257\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.20257\n",
      "\n",
      "Epoch 41: val_loss improved from 1.20257 to 1.19977, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.19977\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.19977\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.19977\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.19977\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.19977\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 47: val_loss improved from 1.19977 to 1.19751, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 48: val_loss improved from 1.19751 to 1.19156, saving model to results/MCNET/weights.keras\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.19156\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 78: early stopping\n",
      "Finished training MCNET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_loss.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_loss.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_acc.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/weights.keras (deflated 8%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/train_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/total_loss.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/val_loss.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/val_acc.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/total_acc.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/train_acc.txt (deflated 58%)\r\n",
      "  adding: kaggle/working/AMC/results/MCNET/weights.keras (deflated 27%)\r\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.08789, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 2.08789 to 2.04578, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 2.04578 to 2.01069, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 2.01069 to 1.78463, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.78463 to 1.63537, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.63537 to 1.52714, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.52714 to 1.47085, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.47085 to 1.40448, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.40448 to 1.36820, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 1.36820 to 1.34235, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 1.34235 to 1.33940, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.33940 to 1.30156, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 1.30156 to 1.29445, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.29445\n",
      "\n",
      "Epoch 15: val_loss improved from 1.29445 to 1.28797, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 1.28797 to 1.27782, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.27782 to 1.26679, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 1.26679 to 1.25325, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 19: val_loss improved from 1.25325 to 1.23586, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 1.23586 to 1.22955, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 1.22955 to 1.22152, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.22152\n",
      "\n",
      "Epoch 23: val_loss improved from 1.22152 to 1.21824, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 1.21824 to 1.20398, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 1.20398 to 1.20079, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.20079\n",
      "\n",
      "Epoch 27: val_loss improved from 1.20079 to 1.19805, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.19805\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.19805\n",
      "\n",
      "Epoch 30: val_loss improved from 1.19805 to 1.19585, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 1.19585 to 1.18917, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.18917\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.18917\n",
      "\n",
      "Epoch 34: val_loss improved from 1.18917 to 1.18688, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 1.18688 to 1.18686, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.18686\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.18686\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.18686\n",
      "\n",
      "Epoch 39: val_loss improved from 1.18686 to 1.18105, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.18105\n",
      "\n",
      "Epoch 41: val_loss improved from 1.18105 to 1.18016, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 1.18016 to 1.17747, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.17747\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.17747\n",
      "\n",
      "Epoch 45: val_loss improved from 1.17747 to 1.17030, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.17030\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.17030\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.17030\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.17030\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.17030\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 51: val_loss improved from 1.17030 to 1.16637, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 52: val_loss improved from 1.16637 to 1.16550, saving model to results/PET-CGDNN/weights.keras\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.16550\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 82: early stopping\n",
      "Finished training PET-CGDNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_loss.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_loss.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_acc.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/weights.keras (deflated 27%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/train_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/total_loss.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/val_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/val_acc.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/total_acc.pdf (deflated 29%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/train_acc.txt (deflated 58%)\r\n",
      "  adding: kaggle/working/AMC/results/PET-CGDNN/weights.keras (deflated 13%)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732230744.314192      76 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_1', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1732230768.571618      74 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_1', 4 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.80609, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.80609 to 1.55812, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.55812 to 1.47899, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.47899 to 1.44720, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.44720 to 1.42388, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.42388 to 1.39998, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 7: val_loss improved from 1.39998 to 1.38374, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 1.38374 to 1.34643, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.34643\n",
      "\n",
      "Epoch 10: val_loss improved from 1.34643 to 1.33805, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 1.33805 to 1.32825, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.32825 to 1.32720, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.32720\n",
      "\n",
      "Epoch 14: val_loss improved from 1.32720 to 1.31571, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.31571\n",
      "\n",
      "Epoch 16: val_loss improved from 1.31571 to 1.30584, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.30584\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.30584\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.30584\n",
      "\n",
      "Epoch 20: val_loss improved from 1.30584 to 1.30333, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 1.30333 to 1.30321, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 1.30321 to 1.29395, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.29395\n",
      "\n",
      "Epoch 24: val_loss improved from 1.29395 to 1.29107, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.29107\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.29107\n",
      "\n",
      "Epoch 27: val_loss improved from 1.29107 to 1.28708, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 28: val_loss improved from 1.28708 to 1.28556, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.28556\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.28556\n",
      "\n",
      "Epoch 31: val_loss improved from 1.28556 to 1.28510, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.28510\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.28510\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.28510\n",
      "\n",
      "Epoch 35: val_loss improved from 1.28510 to 1.28133, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.28133\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.28133\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.28133\n",
      "\n",
      "Epoch 39: val_loss improved from 1.28133 to 1.27291, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.27291\n",
      "\n",
      "Epoch 53: val_loss improved from 1.27291 to 1.27150, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.27150\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.27150\n",
      "\n",
      "Epoch 56: val_loss improved from 1.27150 to 1.26938, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.26938\n",
      "\n",
      "Epoch 68: val_loss improved from 1.26938 to 1.26863, saving model to results/ResNet/weights.keras\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.26863\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 98: early stopping\n",
      "Finished training ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_loss.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_loss.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_acc.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/weights.keras (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/val_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/weights.keras (deflated 13%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/train_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/total_loss.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/val_loss.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/val_acc.txt (deflated 60%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/total_acc.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/train_acc.txt (deflated 58%)\r\n",
      "  adding: kaggle/working/AMC/results/ResNet/weights.keras (deflated 15%)\r\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.02676, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 3.02676 to 2.87379, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 2.87379 to 1.54157, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 1.54157 to 1.39299, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.39299 to 1.34912, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 6: val_loss improved from 1.34912 to 1.33393, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.33393\n",
      "\n",
      "Epoch 8: val_loss improved from 1.33393 to 1.31139, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.31139 to 1.28138, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.28138\n",
      "\n",
      "Epoch 11: val_loss improved from 1.28138 to 1.27550, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 1.27550 to 1.25897, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.25897\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.25897\n",
      "\n",
      "Epoch 15: val_loss improved from 1.25897 to 1.24391, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.24391\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.24391\n",
      "\n",
      "Epoch 18: val_loss improved from 1.24391 to 1.21969, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.21969\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.21969\n",
      "\n",
      "Epoch 21: val_loss improved from 1.21969 to 1.18462, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.18462\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.18462\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.18462\n",
      "\n",
      "Epoch 25: val_loss improved from 1.18462 to 1.18121, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.18121\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.18121\n",
      "\n",
      "Epoch 28: val_loss improved from 1.18121 to 1.15902, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.15902\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.15902\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.15902\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.15902\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.15902\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 34: val_loss improved from 1.15902 to 1.12968, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.12968\n",
      "\n",
      "Epoch 43: val_loss improved from 1.12968 to 1.11563, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.11563\n",
      "\n",
      "Epoch 45: val_loss improved from 1.11563 to 1.11471, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.11471\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.11471\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.11471\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.11471\n",
      "\n",
      "Epoch 50: val_loss improved from 1.11471 to 1.10939, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.10939\n",
      "\n",
      "Epoch 57: val_loss improved from 1.10939 to 1.10263, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.10263\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.10263\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.10263\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.10263\n",
      "\n",
      "Epoch 62: val_loss improved from 1.10263 to 1.09930, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.09930\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.09930\n",
      "\n",
      "Epoch 65: val_loss improved from 1.09930 to 1.09492, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.09492\n",
      "\n",
      "Epoch 74: val_loss improved from 1.09492 to 1.09208, saving model to results/TAD/weights.keras\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.09208\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 104: early stopping\n",
      "Finished training TAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/AMC/results/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_loss.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/GRU2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_loss.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/val_acc.txt (deflated 64%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/total_acc.pdf (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/train_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/IC-AMCNet/weights.keras (deflated 20%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_loss.txt (deflated 56%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/LSTM2/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_loss.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_loss.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/total_acc.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/train_acc.txt (deflated 57%)\r\n",
      "updating: kaggle/working/AMC/results/MCLDNN/weights.keras (deflated 8%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/val_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/MCNET/weights.keras (deflated 27%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/total_loss.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/val_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/val_acc.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/total_acc.pdf (deflated 29%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/PET-CGDNN/weights.keras (deflated 13%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/ (stored 0%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/train_loss.txt (deflated 59%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/total_loss.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/val_loss.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/val_acc.txt (deflated 60%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/total_acc.pdf (deflated 28%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/train_acc.txt (deflated 58%)\r\n",
      "updating: kaggle/working/AMC/results/ResNet/weights.keras (deflated 15%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/ (stored 0%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/train_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/total_loss.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/val_loss.txt (deflated 59%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/val_acc.txt (deflated 61%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/total_acc.pdf (deflated 28%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/train_acc.txt (deflated 58%)\r\n",
      "  adding: kaggle/working/AMC/results/TAD/weights.keras (deflated 27%)\r\n"
     ]
    }
   ],
   "source": [
    "for model_, mdl in list(models_dict.items())[8:]:\n",
    "    # train_data\n",
    "    X_train_, X_val_, X_test_ = X_train.copy(), X_val.copy(), X_test.copy()\n",
    "    if model_ in ['GRU2']:\n",
    "        X_train_ = X_train_.swapaxes(2, 1)\n",
    "        X_val_ = X_val_.swapaxes(2, 1)\n",
    "    elif model_ in ['LSTM2']:\n",
    "        X_train_, X_val_, X_test_ = to_amp_phase(X_train_, X_val_, X_test_, 128)\n",
    "        X_train_ = norm_pad_zeros(X_train_[:, :128, :], 128)\n",
    "        X_val_ = norm_pad_zeros(X_val_[:, :128, :], 128)\n",
    "    elif model_ in ['1DCNN-PF']:\n",
    "        X_train_, X_val_, X_test_ = to_amp_phase(X_train_, X_val_, X_test_, 128)\n",
    "\n",
    "        X_train_ = norm_pad_zeros(X_train_[:, :128, :], 128)\n",
    "        X_val_ = norm_pad_zeros(X_val_[:, :128, :], 128)\n",
    "        \n",
    "        X1_train = X_train_[:, :, 0]\n",
    "        X1_val = X_val_[:, :, 0]\n",
    "        X2_train = X_train_[:, :, 1]\n",
    "        X2_val = X_val_[:, :, 1]\n",
    "        X_train_ = [X1_train, X2_train]\n",
    "        X_val_ = [X1_val, X2_val]\n",
    "        \n",
    "    elif model_ in ['CGDNet']:\n",
    "        X_train_ = np.expand_dims(X_train_, axis=1)\n",
    "        X_val_ = np.expand_dims(X_val_, axis=1)\n",
    "        \n",
    "    elif model_ in ['CLDNN']:\n",
    "        X_train_ = np.reshape(X_train_, (-1, 1, 2, 128))\n",
    "        X_val_ = np.reshape(X_val_, (-1, 1, 2, 128))\n",
    "        \n",
    "    elif model_ in ['CLDNN2', 'DenseNet', 'IC-AMCNet', 'ResNet', 'TAD']:\n",
    "        X_train_ = np.expand_dims(X_train_, axis=3)\n",
    "        X_val_ = np.expand_dims(X_val_, axis=3)\n",
    "                               \n",
    "    elif model_ in ['DAE']:\n",
    "        X_train_, X_val_, X_test_ = to_amp_phase(X_train_, X_val_, X_test_, 128)\n",
    "        X_train_[:, :, 0] = l2_normalize(X_train_[:, :, 0])\n",
    "        X_val_[:, :, 0] = l2_normalize(X_val_[:, :, 0])\n",
    "        for i in range(X_train_.shape[0]):\n",
    "            k = 2/(X_train_[i,:,1].max() - X_train_[i,:,1].min())\n",
    "        X_train_[i,:,1]=-1+k*(X_train_[i,:,1]-X_train_[i,:,1].min())\n",
    "        for i in range(X_val_.shape[0]):\n",
    "            k = 2/(X_val_[i,:,1].max() - X_val_[i,:,1].min())\n",
    "        X_val_[i,:,1]=-1+k*(X_val_[i,:,1]-X_val_[i,:,1].min())\n",
    "\n",
    "    elif model_ in ['MCLDNN']:\n",
    "        X1_train = np.expand_dims(X_train_[:, 0, :], axis=2)\n",
    "        X1_val = np.expand_dims(X_val_[:, 0, :], axis=2)\n",
    "        X2_train = np.expand_dims(X_train_[:, 1, :], axis=2)\n",
    "        X2_val = np.expand_dims(X_val_[:, 1, :], axis=2)\n",
    "        X_train_t = np.expand_dims(X_train_, axis=3)\n",
    "        X_val_t = np.expand_dims(X_val_, axis=3)        \n",
    "        X_train_ = [X_train_t, X1_train, X2_train]\n",
    "        X_val_ = [X_val_t, X1_val, X2_val]\n",
    "\n",
    "    elif model_ in ['MCNET']:\n",
    "        X_train_ = np.expand_dims(X_train_, axis=3)\n",
    "        X_val_ = np.expand_dims(X_val_, axis=3)\n",
    "\n",
    "    elif model_ in ['PET-CGDNN']:\n",
    "        X_train_ = X_train_.swapaxes(2, 1)\n",
    "        X_val_ = X_val_.swapaxes(2, 1)\n",
    "        X1_train = X_train_[:, :, 0]\n",
    "        X2_train = X_train_[:, :, 1]\n",
    "        X1_val = X_val_[:, :, 0]\n",
    "        X2_val = X_val_[:, :, 1]\n",
    "        X_train_ = np.expand_dims(X_train_, axis=3)\n",
    "        X_val_ = np.expand_dims(X_val_, axis=3)\n",
    "        X_train_ = [X_train_, X1_train, X2_train]\n",
    "        X_val_ = [X_val_, X1_val, X2_val]\n",
    "    else:\n",
    "        X_train_ = X_train\n",
    "        X_val_ = X_val\n",
    "\n",
    "    if model_ in ['DAE']:\n",
    "        Y_train_ = [Y_train, X_train_]\n",
    "        Y_val_ = [Y_val, X_val_]\n",
    "    else:\n",
    "        Y_train_ = Y_train\n",
    "        Y_val_ = Y_val\n",
    "    \n",
    "    filepath = f'results/{model_}'\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # optimization params\n",
    "    if model_ in ['DAE']:\n",
    "        compile_params = {'optimizer':Adam(),\n",
    "                          'loss':{'xc': 'categorical_crossentropy', 'xd': 'mean_squared_error'},\n",
    "                          'loss_weights':{'xc': 0.1, 'xd': 0.9},\n",
    "                          'metrics':['accuracy', 'mse']}\n",
    "    else:\n",
    "        compile_params = {'optimizer':Adam(),\n",
    "                          'loss':'categorical_crossentropy',\n",
    "                          'metrics':['accuracy']}\n",
    "    model = mdl()\n",
    "    model.compile(**compile_params)\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(filepath + '/weights.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=5, min_lr=0.0000001),\n",
    "        EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='auto', min_delta=0.00001)\n",
    "    ]\n",
    "    history = model.fit(X_train_, Y_train_, batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
    "                        validation_data=[X_val_, Y_val_], callbacks=callbacks)\n",
    "    if model_ == 'DAE':\n",
    "        history.history['accuracy'] = history.history['xc_accuracy']\n",
    "        history.history['val_accuracy'] = history.history['val_xc_accuracy']\n",
    "    mltools.show_history(history, filepath)\n",
    "    print(f'Finished training {model_}')\n",
    "    \n",
    "    !zip -r results.zip /kaggle/working/AMC/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792a8c9",
   "metadata": {
    "papermill": {
     "duration": 0.044834,
     "end_time": "2024-11-21T23:57:53.342666",
     "exception": false,
     "start_time": "2024-11-21T23:57:53.297832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 579184,
     "sourceId": 1047683,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6115507,
     "sourceId": 9945441,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7660.477806,
   "end_time": "2024-11-21T23:57:56.359579",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-21T21:50:15.881773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
